{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\JFerreira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\Users\\JFerreira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JFerreira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import utils\n",
    "from process_data import * \n",
    "import numpy as np\n",
    "logs_path=\"./logs\"\n",
    "\n",
    "with open ('art_filt.txt', 'rb') as fp:\n",
    "    articulos_filtrados = pickle.load(fp)\n",
    "with open ('art_filt_labels', 'rb') as fp:\n",
    "    articulos_filtrados_labels = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features from articles\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_df=0.8,min_df=10)\n",
    "count_vect.fit(articulos_filtrados) #Aprende el vocabulario y le asigna un código a cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count_vect.vocabulary_ #Estos son los índices de cada una de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary=count_vect.get_feature_names() #Estos nombres de las palabras seleccionadas para el vocabulario, ordenadas por orden alfabético\n",
    "#print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_data=count_vect.fit_transform(articulos_filtrados) #Aprende el vocabulario y le asigna un código a cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(X_train_data.shape) #Para cada documento hay un vector de ocurrencias\n",
    "X_train_data=X_train_data.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "#labels=tf.one_hot(articulos_filtrados_labels, 20)\n",
    "labels = np.zeros((X_train_data.shape[0], 20))\n",
    "labels[np.arange(X_train_data.shape[0]), articulos_filtrados_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Parameters\n",
    "# learning_rate = 0.01\n",
    "# training_epochs = 15\n",
    "# batch_size = 10\n",
    "# display_step = 1\n",
    "\n",
    "# # Network Parameters\n",
    "# n_input =  X_train_data.shape[1] # Twenty groups dataset\n",
    "# n_classes = 20 # Twenty groups total classes\n",
    "\n",
    "# # tf Graph input\n",
    "# X = tf.placeholder(\"float\", [None, n_input])\n",
    "# Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# # Store layers weight & bias\n",
    "\n",
    "# weights= tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "# bias= tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "\n",
    "# logits = tf.matmul(X, weights) + bias\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#     logits=logits, labels=Y))\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# train_op = optimizer.minimize(loss_op)\n",
    "# # Initializing the variables\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     # Training cycle\n",
    "#     total_batch = int(X_train_data.shape[0]/batch_size)\n",
    "#     summary_writer = tf.summary.FileWriter(logs_path, graph=sess.graph)\n",
    "#     for epoch in range(training_epochs):\n",
    "#         avg_cost = 0.\n",
    "#         # Loop over all batches\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x, batch_y = next_batch(batch_size,X_train_data,labels)\n",
    "#             # Run optimization op (backprop) and cost op (to get loss value)\n",
    "#             _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "#                                                             Y: batch_y})\n",
    "#             # Compute average loss\n",
    "#             avg_cost += c / total_batch\n",
    "#         # Display logs per epoch step\n",
    "#         if epoch % display_step == 0:\n",
    "#             print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "#     print(\"Optimization Finished!\")\n",
    "\n",
    "#     # Test model\n",
    "#     pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#     print(\"Accuracy:\", accuracy.eval({X: X_train_data, Y: labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=5.206035007\n",
      "Epoch: 0002 cost=2.332145100\n",
      "Epoch: 0003 cost=1.374946593\n",
      "Epoch: 0004 cost=0.917003932\n",
      "Epoch: 0005 cost=0.661295883\n",
      "Epoch: 0006 cost=0.497647485\n",
      "Epoch: 0007 cost=0.363469723\n",
      "Epoch: 0008 cost=0.299488028\n",
      "Epoch: 0009 cost=0.235374669\n",
      "Epoch: 0010 cost=0.193180431\n",
      "Epoch: 0011 cost=0.150292967\n",
      "Epoch: 0012 cost=0.127669681\n",
      "Epoch: 0013 cost=0.112089286\n",
      "Epoch: 0014 cost=0.093388184\n",
      "Epoch: 0015 cost=0.080045483\n",
      "Epoch: 0016 cost=0.069234448\n",
      "Epoch: 0017 cost=0.059494326\n",
      "Epoch: 0018 cost=0.052021283\n",
      "Epoch: 0019 cost=0.050697736\n",
      "Epoch: 0020 cost=0.043760210\n",
      "Epoch: 0021 cost=0.040146294\n",
      "Epoch: 0022 cost=0.038285666\n",
      "Epoch: 0023 cost=0.032514383\n",
      "Epoch: 0024 cost=0.033092695\n",
      "Epoch: 0025 cost=0.031995517\n",
      "Epoch: 0026 cost=0.026801304\n",
      "Epoch: 0027 cost=0.021715480\n",
      "Epoch: 0028 cost=0.022000365\n",
      "Epoch: 0029 cost=0.020576401\n",
      "Epoch: 0030 cost=0.018049895\n",
      "Epoch: 0031 cost=0.019142107\n",
      "Epoch: 0032 cost=0.017890235\n",
      "Epoch: 0033 cost=0.016263869\n",
      "Epoch: 0034 cost=0.016419759\n",
      "Epoch: 0035 cost=0.015894747\n",
      "Epoch: 0036 cost=0.015210211\n",
      "Epoch: 0037 cost=0.014031127\n",
      "Epoch: 0038 cost=0.012908942\n",
      "Epoch: 0039 cost=0.012372622\n",
      "Epoch: 0040 cost=0.012732969\n",
      "Epoch: 0041 cost=0.010980853\n",
      "Epoch: 0042 cost=0.009188847\n",
      "Epoch: 0043 cost=0.010047187\n",
      "Epoch: 0044 cost=0.010891695\n",
      "Epoch: 0045 cost=0.008740244\n",
      "Epoch: 0046 cost=0.009109146\n",
      "Epoch: 0047 cost=0.007974908\n",
      "Epoch: 0048 cost=0.007183848\n",
      "Epoch: 0049 cost=0.006640543\n",
      "Epoch: 0050 cost=0.007446500\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9991161\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "from tensorboard import summary as summary_lib\n",
    "shutil.rmtree(logs_path)\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 50\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "hidden_units=50\n",
    "\n",
    "# Network Parameters\n",
    "n_input =  X_train_data.shape[1] # Vocab size \n",
    "n_classes = 20 # Twenty news groups # classes\n",
    "\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, n_input],name=\"X\")\n",
    "with tf.name_scope(\"labels\"):\n",
    "    Y = tf.placeholder(\"float\", [None, n_classes],name=\"Y\")\n",
    "\n",
    "with tf.name_scope('Capa1'):\n",
    "    # Model\n",
    "    weights1= tf.Variable(tf.random_normal([n_input, hidden_units]),name=\"weights1\")\n",
    "    bias1= tf.Variable(tf.random_normal([hidden_units]),name=\"bias1\")\n",
    "    act1= tf.nn.sigmoid(tf.matmul(X,weights1)+bias1, name=\"activacion_1\")\n",
    "\n",
    "with tf.name_scope('Capa2'):\n",
    "    # Model\n",
    "    weights2= tf.Variable(tf.random_normal([hidden_units, n_classes]),name=\"weights2\")\n",
    "    bias2= tf.Variable(tf.random_normal([n_classes]),name=\"bias2\")\n",
    "    logits= tf.matmul(act1,weights2)+bias2\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "# Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y),name=\"costo\")\n",
    "\n",
    "with tf.name_scope('BGD'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,name=\"optimizador\")\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    #pred = tf.nn.softmax(logits) # Softmax\n",
    "    acc_op = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(acc_op, tf.float32),name=\"acc_red_mean\")\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss_op)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", acc_op)\n",
    "# Merge all summaries into a single op\n",
    "tf.summary.histogram('histogram', weights1)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=sess.graph)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(X_train_data.shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next_batch(batch_size,X_train_data,labels)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c= sess.run([train_op, loss_op], feed_dict={Y: batch_y,\n",
    "                                                            X: batch_x})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            batch_x, batch_y = next_batch(batch_size,X_train_data,labels)\n",
    "            #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            #run_metadata = tf.RunMetadata()\n",
    "            summary, _,_ = sess.run([merged_summary_op,loss_op,acc_op],\n",
    "                                  feed_dict={X: X_train_data, Y: labels})#,\n",
    "                                  #options=run_options,\n",
    "                                  #run_metadata=run_metadata)\n",
    "            summary_writer.add_summary(summary, epoch)\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: X_train_data, Y: labels})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
