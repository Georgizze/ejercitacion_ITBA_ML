{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143613, 8)\n",
      "(15958, 8)\n"
     ]
    }
   ],
   "source": [
    "# cargo los datasets\n",
    "folder = 'C:/Users/JFerreira/Documents/01 - BI, Big Data and Analytics/ITBA_ML/ejercitacion_ITBA_ML/multiclass/toxic_comments/'\n",
    "\n",
    "train = pd.read_csv(folder+\"train.csv\") #set para entrenamiento y validacion\n",
    "test = pd.read_csv(folder+\"test.csv\") #set de prueba con el que evaluan el mejor modelo\n",
    "test_labels = pd.read_csv(folder+\"test_labels.csv\") #labels del set de prueba (lo agregan luego de que finaliza la competencia)\n",
    "submission = pd.read_csv(folder+\"sample_submission.csv\") #formato del submmit para kaggle\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "\n",
    "# me quedo solo con los comentarios (X)\n",
    "#raw_text_complete = train[\"comment_text\"].str.lower()\n",
    "\n",
    "# Separo el set en train y validation set\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(train, y, test_size = 0.1)\n",
    "\n",
    "# me quedo solo con los comentarios (X)\n",
    "raw_text_train = X_train[\"comment_text\"].str.lower()\n",
    "raw_text_valid = X_valid[\"comment_text\"].str.lower()\n",
    "raw_text_test = test[\"comment_text\"].str.lower()\n",
    "\n",
    "# veo cuantos datos me quedaron en cada set\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension del TRAIN set original:  (143613, 70112)\n",
      "Dimension del VALID set original:  (15958, 70112)\n"
     ]
    }
   ],
   "source": [
    "# armo la matriz de features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_features = 100000 # cantidad de palabras (features) que voy a usar para entrenar\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, # ignorar palabras con una frecuencia mayor al 95%\n",
    "                                   min_df=2, # ignorar palabras con una frecuencia menor a 2\n",
    "                                   max_features=max_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(raw_text_train) #datos de entrenamiento\n",
    "tfidf_matrix_valid = tfidf_vectorizer.transform(raw_text_valid) #datos de validacion\n",
    "\n",
    "# PREPROCESAMIENTO Y DIMENSIONALITY REDUCTION\n",
    "print(\"Dimension del TRAIN set original: \", tfidf_matrix_train.shape)\n",
    "print(\"Dimension del VALID set original: \", tfidf_matrix_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension TRAIN set luego de Random Projection:  (143613, 10178)\n",
      "Dimension VALID set luego de Random Projection:  (15958, 10178)\n"
     ]
    }
   ],
   "source": [
    "# RANDOM PROJECTION\n",
    "from sklearn import random_projection\n",
    "\n",
    "transformer = random_projection.SparseRandomProjection()\n",
    "X_train_new = transformer.fit_transform(tfidf_matrix_train)\n",
    "X_valid_new = transformer.transform(tfidf_matrix_valid)\n",
    "\n",
    "print(\"Dimension TRAIN set luego de Random Projection: \", X_train_new.shape)\n",
    "print(\"Dimension VALID set luego de Random Projection: \", X_valid_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-04792bb55c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparsePCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_valid_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\decomposition\\sparse_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \"\"\"\n\u001b[1;32m    117\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 431\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    276\u001b[0m                         \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "# INCREMENTAL PCA (Principal Components Analysis)\n",
    "from  sklearn.decomposition import SparsePCA\n",
    "\n",
    "pca = SparsePCA(n_components = 0.9)\n",
    "\n",
    "X_train_final = pca.fit_transform(X_train_new)\n",
    "X_valid_final = pca.transform(X_valid_new)\n",
    "\n",
    "print(\"Dimension TRAIN set luego de PCA: \", X_train_new.shape)\n",
    "print(\"Dimension VALID set luego de PCA: \", X_valid_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# freatures names\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# top X palabras mas usadas\n",
    "#topx = 10\n",
    "#top_x = np.argsort(tfidf_matrix_train.sum(axis=0))[0,::-1][0,:topx].tolist()[0]\n",
    "#feature_names[np.array(top_x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo regresion logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# AUC for a binary classifier\n",
    "def auc(y_true, y_pred):   \n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 30)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 30)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)    \n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)    \n",
    "    return TP/P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Activation,  Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from helper import PlotLosses\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_features = dense_matrix_train.shape[1]\n",
    "output_size = Y_train.shape[1]\n",
    "hidden_units = 300\n",
    "hidden_units_2 = 300\n",
    "hidden_units_3 = 300\n",
    "lambd = 0 #0.001\n",
    "prob = 0.5\n",
    "\n",
    "#modelo\n",
    "model_sig_nn = Sequential()\n",
    "# capa oculta 1\n",
    "model_sig_nn.add(Dense(hidden_units,input_dim=input_features, kernel_regularizer=regularizers.l2(lambd), \n",
    "                       kernel_initializer=\"he_uniform\", name=\"Capa_Oculta_1\"))\n",
    "model_sig_nn.add(BatchNormalization())\n",
    "model_sig_nn.add(Activation('elu', name=\"output_capa_oculta_1\")) \n",
    "model_sig_nn.add(Dropout(prob))\n",
    "# capa oculta 2\n",
    "model_sig_nn.add(Dense(hidden_units_2,input_dim=hidden_units, kernel_regularizer=regularizers.l2(lambd), \n",
    "                       kernel_initializer=\"he_uniform\",name=\"Capa_Oculta_2\"))\n",
    "model_sig_nn.add(BatchNormalization())\n",
    "model_sig_nn.add(Activation('elu', name=\"output_capa_oculta_2\")) \n",
    "model_sig_nn.add(Dropout(prob))\n",
    "# capa oculta 3\n",
    "#model_sig_nn.add(Dense(hidden_units_3,input_dim=hidden_units_2, kernel_regularizer=regularizers.l2(lambd), \n",
    "#                       kernel_initializer=\"he_uniform\",name=\"Capa_Oculta_3\"))\n",
    "#model_sig_nn.add(BatchNormalization())\n",
    "#model_sig_nn.add(Activation('elu', name=\"output_capa_oculta_3\")) \n",
    "#model_sig_nn.add(Dropout(prob))\n",
    "# capa de salida\n",
    "model_sig_nn.add(Dense(output_size,input_dim=hidden_units_2, kernel_regularizer=regularizers.l2(lambd), name=\"Capa_Salida\"))\n",
    "model_sig_nn.add(Activation('sigmoid', name=\"output\"))\n",
    "model_sig_nn.summary()\n",
    "\n",
    "lr = 0.01 \n",
    "batch_size = 2048\n",
    "epochs = 5\n",
    "\n",
    "selectedOptimizer = optimizers.adam(lr=lr)\n",
    "\n",
    "model_sig_nn.compile(loss = 'binary_crossentropy', \n",
    "                     optimizer=selectedOptimizer, \n",
    "                     metrics=['accuracy']\n",
    "                    ) #auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='basic_model_best.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "plot_losses = PlotLosses(plot_interval=1, \n",
    "                         evaluate_interval=50, \n",
    "                         x_val=dense_matrix_valid, \n",
    "                         y_val_categorical=Y_valid)\n",
    "\n",
    "history = model_sig_nn.fit(dense_matrix_train, \n",
    "          Y_train, \n",
    "          batch_size = batch_size,\n",
    "          epochs=epochs, \n",
    "          verbose=1, \n",
    "          validation_data=(dense_matrix_valid, Y_valid), \n",
    "          callbacks=[plot_losses, checkpointer],\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluo set de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_sig_nn.load_weights('basic_model_best.hdf5')\n",
    "#tfidf_matrix_valid.shape, Y_valid.shape\n",
    "\n",
    "pred_valid = model_sig_nn.predict(dense_matrix_valid, verbose = 1)\n",
    "pred_train = model_sig_nn.predict(dense_matrix_train, verbose = 1)\n",
    "\n",
    "# loss y accuracy\n",
    "#model_sig_nn.evaluate(dense_matrix_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "print(\"TRAIN ROC AUC: \", roc_auc_score(Y_train, pred_train, average='macro'))\n",
    "print(\"VALID ROC AUC: \", roc_auc_score(Y_valid, pred_valid, average='macro'))\n",
    "\n",
    "# 300 300 6000 6\n",
    "#TRAIN ROC AUC:  0.9912458593862464\n",
    "#VALID ROC AUC:  0.9778854055090332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grafico las curvas de roc general y de cada clase\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = Y_valid.shape[1]\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_valid[:, i], pred_valid[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_valid.ravel(), pred_valid.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "lw = 2\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive Rate**:   \n",
    "The number of times your system was able to classify the positives as positives. \n",
    "\n",
    "True positive rate = Correctly Classified Positives/(Correctly Classified as Positives+ Falsely Classified as Negatives)\n",
    "\n",
    "**False Positive Rate**:  \n",
    "The number of times your system classified a negative as a positive divided by the total  actual negative instances.\n",
    "\n",
    "\n",
    "False positive rate = Incorrectly Classified as Positives/(Incorrectly Classified as Positives+ Correctly classified as Negatives )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "\n",
    "Macro-average: Calcula el score de cada clase y luego promedia  \n",
    "Micro-average: Suma y luego calcula el score\n",
    "\n",
    "Micro-average se considera mejor cuando hay desbalce en las clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix_test = tfidf_vectorizer.transform(raw_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_matrix_test = tfidf_matrix_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model_sig_nn.predict(dense_matrix_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1*(pred[0:10]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission[list_classes] = pred\n",
    "submission.to_csv(\"submission_early_stop_2_epochs.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST ROC AUC: 0.9673"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
